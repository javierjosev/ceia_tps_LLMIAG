{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs e IA Generativa\n",
    "\n",
    "## Chatbot de CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del entorno\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import PyPDF2\n",
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Pinecone\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga y procesamiento de PDFs\n",
    "pdf_dir = \"docs\"  # Directorio donde se almacenan los PDFs\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar todos los PDFs\n",
    "documents = []\n",
    "for pdf in pdf_files:\n",
    "    text = extract_text_from_pdf(os.path.join(pdf_dir, pdf))\n",
    "    documents.append({\"filename\": pdf, \"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name=\"all-MiniLM-L6-v2\"\n",
    "embedding_model_dim=384\n",
    "\n",
    "embedding_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "def generate_embedding(text):\n",
    "    return embedding_model.encode(text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index with name cvs-embeddings already created. Skipping!\n"
     ]
    }
   ],
   "source": [
    "# Valida si el index existe o no\n",
    "\n",
    "index_name = \"cvs-embeddings\"\n",
    "\n",
    "existing_indexes = pc.list_indexes()\n",
    "index_names = [index.name for index in existing_indexes]\n",
    "\n",
    "if index_name not in index_names:\n",
    "    print(\"Index doesn't exist. Creating!\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=embedding_model_dim,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ) \n",
    "    )\n",
    "else:\n",
    "    print(\"Index with name \" + index_name + \" already created. Skipping!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document: Javier Villagra - Resume.pdf\n",
      "File 'Javier Villagra - Resume.pdf' found on index. Upserting aborted...\n"
     ]
    }
   ],
   "source": [
    "# Cargando el index de Pinecone\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Documentos cargados en el directorio\n",
    "documents_names = [document['filename'] for document in documents]\n",
    "\n",
    "# Documentos cargados en Pinecone\n",
    "existing_indexes = pc.list_indexes()\n",
    "index_names = [index.name for index in existing_indexes]\n",
    "\n",
    "\n",
    "for document_name in documents_names:\n",
    "    print(\"Processing document: \" + document_name)\n",
    "\n",
    "    dummy_vector = [0.0] * 384\n",
    "\n",
    "    # Realiza la búsqueda con el filtro en el campo 'filename' de la metadata\n",
    "    query_results = index.query(\n",
    "        vector=dummy_vector,\n",
    "        top_k=10,  # Número máximo de resultados a devolver\n",
    "        filter={'filename': {'$eq': document_name}},  # Filtro de metadata\n",
    "        include_metadata=True  # Incluir metadata en los resultados\n",
    "    )\n",
    "\n",
    "    # # Imprime los resultados\n",
    "    # for result in query_results['matches']:\n",
    "    #     print(f\"ID: {result['id']}\")\n",
    "    #     print(f\"Filename: {result['metadata']['filename']}\")\n",
    "    #     print(f\"Score: {result['score']}\")\n",
    "\n",
    "    if query_results['matches']:\n",
    "        print(f\"File '{document_name}' found on index. Upserting aborted...\")\n",
    "    else:\n",
    "        print(f\"File '{document_name}' not found. Initializing upserting!!..\")\n",
    "\n",
    "        # Chunking recursivo\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        chunks = []\n",
    "\n",
    "        doc = next((doc for doc in documents if doc['filename'] == document_name), None)\n",
    "\n",
    "        doc_chunks = text_splitter.split_text(doc[\"text\"])\n",
    "        chunks.extend([{\"filename\": doc[\"filename\"], \"chunk\": chunk} for chunk in doc_chunks])\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk[\"embedding\"] = generate_embedding(chunk[\"chunk\"])\n",
    "\n",
    "        # Subir los vectores a Pinecone\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            index.upsert([(str(i), chunk[\"embedding\"], {\"filename\": chunk[\"filename\"], \"chunk\": chunk[\"chunk\"]})])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest chunk:\n",
      "{'matches': [{'id': '8',\n",
      "              'metadata': {'chunk': \"with the channel's ESB.\\n\"\n",
      "                                    'Conducted effective troubleshooting and '\n",
      "                                    'resolved issues with coding, design and '\n",
      "                                    'infrastructure.Banco Macro, Argentina\\n'\n",
      "                                    'Page 1Universidad de Buenos AiresSYSTEMS '\n",
      "                                    'ENGINEERING2001 - 2008Microservices\\n'\n",
      "                                    'Java EEAgile Management\\n'\n",
      "                                    'Fintech\\n'\n",
      "                                    'Universidad de Buenos AiresPOSTGRADUATE '\n",
      "                                    'DEGREE,\\n'\n",
      "                                    'AI SPECIALIST2023 - Machine '\n",
      "                                    'LearningArtificial Intelligence\\n'\n",
      "                                    'Deep Learning\\n'\n",
      "                                    'PythonEXPERIENCE\\n'\n",
      "                                    'LANGUAGE\\n'\n",
      "                                    'Native or bilingual proficiencySPANISH\\n'\n",
      "                                    'Professional working proficiencyENGLISH',\n",
      "                           'filename': 'Javier Villagra - Resume.pdf'},\n",
      "              'score': 0.326605469,\n",
      "              'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 6}}\n"
     ]
    }
   ],
   "source": [
    "# Probar una consulta\n",
    "def find_similar(query):\n",
    "    query_embedding = generate_embedding(query)\n",
    "    response = index.query(vector=query_embedding, top_k=1, include_metadata=True)\n",
    "    # sample: index.query(vector=[0.1, 0.2, 0.3], top_k=10, namespace='my_namespace')\n",
    "    return response\n",
    "\n",
    "\n",
    "query = \"Does he know Java?\"\n",
    "response = find_similar(query)\n",
    "\n",
    "print(\"Closest chunk:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A nice question!\n",
      "\n",
      "According to the information provided, the answer is: Yes, he knows Java!\n",
      "\n",
      "Not only does he have experience with Java EE (Enterprise Edition), but he also mentions Microservices, which is a design pattern often associated with Java development. Additionally, he lists Java EE as one of his skills, which indicates that he has a strong background in Java programming.\n"
     ]
    }
   ],
   "source": [
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "client = Groq(\n",
    "    api_key=groq_api_key,\n",
    ")\n",
    "\n",
    "context = response['matches'][0]['metadata']['chunk']\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Answer this question: '\"+query+\"' using this information: \" + context,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
