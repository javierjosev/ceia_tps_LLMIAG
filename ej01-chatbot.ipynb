{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs e IA Generativa\n",
    "\n",
    "## Chatbot de CVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio se desarrolla un chatbot basado en un sistema de Retrieval Augmented Generation (RAG). Este utiliza todos los archivos PDF almacenados en el directorio \"/docs\" como fuente de datos, los cuales se emplean para proporcionar contexto al modelo de lenguaje (LLM) y permitirle responder de manera más precisa a las preguntas planteadas.\n",
    "\n",
    "Para la codificación de los documentos, se emplea una estrategia de chunking recursivo. Este enfoque divide los documentos en fragmentos más pequeños, optimizando así la eficiencia en el procesamiento de texto. Se utiliza el modelo \"all-MiniLM-L6-v2\" para realizar el embedding de los fragmentos. A continuación, tanto embeddings como fragmentos son persistidos en una base de datos Pinecone. Para la generación de respuestas, se utiliza el modelo de lenguaje \"llama3-8b-8192\", que aprovecha el contexto proporcionado por los datos almacenados en Pinecone para generar las respuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuración del entorno\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import PyPDF2\n",
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuración de la base de datos de Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Pinecone\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga y procesamiento de los PDFs presentes en el directorio \"/docs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga y procesamiento de PDFs\n",
    "pdf_dir = \"docs\"  # Directorio donde se almacenan los PDFs\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar todos los PDFs\n",
    "documents = []\n",
    "for pdf in pdf_files:\n",
    "    text = extract_text_from_pdf(os.path.join(pdf_dir, pdf))\n",
    "    documents.append({\"filename\": pdf, \"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name=\"all-MiniLM-L6-v2\"\n",
    "embedding_model_dim=384\n",
    "\n",
    "embedding_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "def generate_embedding(text):\n",
    "    return embedding_model.encode(text).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generación del index y posterior upserting de los embeddings en Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index doesn't exist. Creating!\n"
     ]
    }
   ],
   "source": [
    "# Valida si el index existe o no\n",
    "\n",
    "index_name = \"cvs-embeddings\"\n",
    "\n",
    "existing_indexes = pc.list_indexes()\n",
    "index_names = [index.name for index in existing_indexes]\n",
    "\n",
    "if index_name not in index_names:\n",
    "    print(\"Index doesn't exist. Creating!\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=embedding_model_dim,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ) \n",
    "    )\n",
    "else:\n",
    "    print(\"Index with name \" + index_name + \" already created. Skipping!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document: Javier Villagra - Resume.pdf\n",
      "File 'Javier Villagra - Resume.pdf' not found. Initializing upserting!!..\n"
     ]
    }
   ],
   "source": [
    "# Cargando el index de Pinecone\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Documentos cargados en el directorio\n",
    "documents_names = [document['filename'] for document in documents]\n",
    "\n",
    "# Documentos cargados en Pinecone\n",
    "existing_indexes = pc.list_indexes()\n",
    "index_names = [index.name for index in existing_indexes]\n",
    "\n",
    "\n",
    "for document_name in documents_names:\n",
    "    print(\"Processing document: \" + document_name)\n",
    "\n",
    "    dummy_vector = [0.0] * 384\n",
    "\n",
    "    # Realiza la búsqueda con el filtro en el campo 'filename' de la metadata\n",
    "    query_results = index.query(\n",
    "        vector=dummy_vector,\n",
    "        top_k=10,  # Número máximo de resultados a devolver\n",
    "        filter={'filename': {'$eq': document_name}},  # Filtro de metadata\n",
    "        include_metadata=True  # Incluir metadata en los resultados\n",
    "    )\n",
    "\n",
    "    # # Imprime los resultados\n",
    "    # for result in query_results['matches']:\n",
    "    #     print(f\"ID: {result['id']}\")\n",
    "    #     print(f\"Filename: {result['metadata']['filename']}\")\n",
    "    #     print(f\"Score: {result['score']}\")\n",
    "\n",
    "    if query_results['matches']:\n",
    "        print(f\"File '{document_name}' found on index. Upserting aborted...\")\n",
    "    else:\n",
    "        print(f\"File '{document_name}' not found. Initializing upserting!!..\")\n",
    "\n",
    "        # Chunking recursivo\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        chunks = []\n",
    "\n",
    "        doc = next((doc for doc in documents if doc['filename'] == document_name), None)\n",
    "\n",
    "        doc_chunks = text_splitter.split_text(doc[\"text\"])\n",
    "        chunks.extend([{\"filename\": doc[\"filename\"], \"chunk\": chunk} for chunk in doc_chunks])\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk[\"embedding\"] = generate_embedding(chunk[\"chunk\"])\n",
    "\n",
    "        # Subir los vectores a Pinecone\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            index.upsert([(str(i), chunk[\"embedding\"], {\"filename\": chunk[\"filename\"], \"chunk\": chunk[\"chunk\"]})])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta por similaridad coseno sobre la base de datos de Pinecone para obtener el contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest chunk:\n",
      "{'matches': [{'id': '8',\n",
      "              'metadata': {'chunk': \"with the channel's ESB.\\n\"\n",
      "                                    'Conducted effective troubleshooting and '\n",
      "                                    'resolved issues with coding, design and '\n",
      "                                    'infrastructure.Banco Macro, Argentina\\n'\n",
      "                                    'Page 1Universidad de Buenos AiresSYSTEMS '\n",
      "                                    'ENGINEERING2001 - 2008Microservices\\n'\n",
      "                                    'Java EEAgile Management\\n'\n",
      "                                    'Fintech\\n'\n",
      "                                    'Universidad de Buenos AiresPOSTGRADUATE '\n",
      "                                    'DEGREE,\\n'\n",
      "                                    'AI SPECIALIST2023 - Machine '\n",
      "                                    'LearningArtificial Intelligence\\n'\n",
      "                                    'Deep Learning\\n'\n",
      "                                    'PythonEXPERIENCE\\n'\n",
      "                                    'LANGUAGE\\n'\n",
      "                                    'Native or bilingual proficiencySPANISH\\n'\n",
      "                                    'Professional working proficiencyENGLISH',\n",
      "                           'filename': 'Javier Villagra - Resume.pdf'},\n",
      "              'score': 0.325969,\n",
      "              'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 6}}\n"
     ]
    }
   ],
   "source": [
    "# Probar una consulta\n",
    "def find_similar(query):\n",
    "    query_embedding = generate_embedding(query)\n",
    "    response = index.query(vector=query_embedding, top_k=1, include_metadata=True)\n",
    "    # sample: index.query(vector=[0.1, 0.2, 0.3], top_k=10, namespace='my_namespace')\n",
    "    return response\n",
    "\n",
    "\n",
    "query = \"Does he know Java?\"\n",
    "response = find_similar(query)\n",
    "\n",
    "print(\"Closest chunk:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generación de la consulta al LLM empleando como contexto los datos devueltos por Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, we can conclude that the individual has some experience with Java, specifically with Java EE, which is a Java technology developed by Sun Microsystems and now owned by Oracle. He has also listed \"Java\" as a skill under his \"Languages\" section, but without specifying the level of proficiency.\n",
      "\n",
      "It is important to note that the individual also has experience with Python and has listed it as a language under his \"EXPERIENCE\" section. However, we cannot conclude that he does not know Java based on this information.\n",
      "\n",
      "To answer the original question, we can say that:\n",
      "\"Based on the provided information, it is not explicitly stated that he does not know Java, and he has listed it as a skill, but the level of his proficiency is not specified. It is also worth noting that he has experience with Python and has listed it as a language.\"\n"
     ]
    }
   ],
   "source": [
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "client = Groq(\n",
    "    api_key=groq_api_key,\n",
    ")\n",
    "\n",
    "context = response['matches'][0]['metadata']['chunk']\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Answer this question: '\"+query+\"' using this information: \" + context,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
